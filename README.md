# Inverter Performance Chatbot

This repository implements a multi-agent chatbot that analyzes inverter performance data, including predicted downtime, failure probabilities, and feature importance. It supports both CLI and REST API access, powered by LangChain and Ollama LLM.

---

## Features

- ğŸ” Understands natural language queries like:
  - "Which inverter has the highest hourly failure probability?"
  - "Rank inverters by predicted downtime in May 2024"
  - "What factors contribute most to inverter failure?"

- ğŸ“Š Dynamically selects relevant dataset (`performance`, `features`)
- ğŸ§  Aggregates and analyzes metrics using Python tools
- ğŸ§¾ Converts numeric results into concise natural language responses
- ğŸ’¬ Supports CLI and REST API usage

---

## Datasets

Place your data files in `data/`:

- `inverter_performance_prediction.csv`
- `feature_importance.csv`

---

## Usage

### CLI

```bash
python cli.py
```

Then enter your natural language question.

### REST API

```bash
uvicorn main:app --reload
```

Send a POST request to:

```http
POST http://localhost:8000/query
{
  "query": "Which inverter has the highest failure probability in May 2024?"
}
```

---

## Structure

```
inverter_chatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents.py           # LangChain agent setup using tools and shared context
â”‚   â”œâ”€â”€ cli.py              # CLI interface to interact with the chatbot
â”‚   â”œâ”€â”€ main.py             # FastAPI app for REST API interaction
â”‚   â”œâ”€â”€ shared.py           # Loads CSVs and stores shared_context and dataset metadata
â”‚   â””â”€â”€ tools.py            # Tool definitions for data extraction, aggregation, analysis
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ inverter_performance_prediction.csv
â”‚   â””â”€â”€ feature_importance.csv
â”œâ”€â”€ README.md               # Documentation and instructions
â”œâ”€â”€ requirements.txt        # Python dependencies

```

---

## Requirements

- Python 3.10+
- `pip install -r requirements.txt`

Ensure you have [Ollama](https://ollama.com/) installed and running a model like `llama3`.

---

## Acknowledgment

Built using LangChain, FastAPI, and Ollama for local language modeling.